"""\nMNIST Image Classifier using Convolutional Neural Network\nBuilt with PyTorch for high-performance deep learning\n\nAuthor: Solvyr Eryx\nProject: AI-DS-Masterpiece\n"""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm import tqdm\nimport time\n\n# Set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Using device: {device}')\n\n\nclass EnhancedCNN(nn.Module):\n    \"\"\"\n    Advanced Convolutional Neural Network for MNIST Classification\n    Architecture: Conv -> ReLU -> Pool -> Conv -> ReLU -> Pool -> FC -> Dropout -> FC\n    \"\"\"\n    def __init__(self):\n        super(EnhancedCNN, self).__init__()\n        \n        # Convolutional layers\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        \n        # Batch normalization\n        self.bn1 = nn.BatchNorm2d(32)\n        self.bn2 = nn.BatchNorm2d(64)\n        self.bn3 = nn.BatchNorm2d(128)\n        \n        # Pooling\n        self.pool = nn.MaxPool2d(2, 2)\n        \n        # Fully connected layers\n        self.fc1 = nn.Linear(128 * 3 * 3, 256)\n        self.fc2 = nn.Linear(256, 128)\n        self.fc3 = nn.Linear(128, 10)\n        \n        # Dropout for regularization\n        self.dropout = nn.Dropout(0.5)\n        \n    def forward(self, x):\n        # Layer 1: Conv -> BN -> ReLU -> Pool\n        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n        \n        # Layer 2: Conv -> BN -> ReLU -> Pool\n        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n        \n        # Layer 3: Conv -> BN -> ReLU -> Pool\n        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n        \n        # Flatten\n        x = x.view(-1, 128 * 3 * 3)\n        \n        # Fully connected layers\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = F.relu(self.fc2(x))\n        x = self.dropout(x)\n        x = self.fc3(x)\n        \n        return x\n\n\ndef load_data(batch_size=64):\n    \"\"\"\n    Load and preprocess MNIST dataset\n    \"\"\"\n    # Data transformations\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))  # MNIST mean and std\n    ])\n    \n    # Download and load training data\n    train_dataset = datasets.MNIST(\n        root='./data',\n        train=True,\n        download=True,\n        transform=transform\n    )\n    \n    # Download and load test data\n    test_dataset = datasets.MNIST(\n        root='./data',\n        train=False,\n        download=True,\n        transform=transform\n    )\n    \n    # Create data loaders\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=2\n    )\n    \n    test_loader = DataLoader(\n        test_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=2\n    )\n    \n    return train_loader, test_loader\n\n\ndef train_model(model, train_loader, criterion, optimizer, device):\n    \"\"\"\n    Train the model for one epoch\n    \"\"\"\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    \n    pbar = tqdm(train_loader, desc='Training')\n    for images, labels in pbar:\n        images, labels = images.to(device), labels.to(device)\n        \n        # Zero the gradients\n        optimizer.zero_grad()\n        \n        # Forward pass\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        \n        # Backward pass and optimization\n        loss.backward()\n        optimizer.step()\n        \n        # Statistics\n        running_loss += loss.item()\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n        \n        # Update progress bar\n        pbar.set_postfix({\n            'loss': f'{running_loss/len(pbar):.4f}',\n            'acc': f'{100*correct/total:.2f}%'\n        })\n    \n    epoch_loss = running_loss / len(train_loader)\n    epoch_acc = 100 * correct / total\n    \n    return epoch_loss, epoch_acc\n\n\ndef evaluate_model(model, test_loader, criterion, device):\n    \"\"\"\n    Evaluate the model on test data\n    \"\"\"\n    model.eval()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for images, labels in tqdm(test_loader, desc='Evaluating'):\n            images, labels = images.to(device), labels.to(device)\n            \n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            \n            running_loss += loss.item()\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    \n    test_loss = running_loss / len(test_loader)\n    test_acc = 100 * correct / total\n    \n    return test_loss, test_acc\n\n\ndef visualize_predictions(model, test_loader, device, num_images=10):\n    \"\"\"\n    Visualize model predictions\n    \"\"\"\n    model.eval()\n    \n    images, labels = next(iter(test_loader))\n    images, labels = images[:num_images].to(device), labels[:num_images]\n    \n    with torch.no_grad():\n        outputs = model(images)\n        _, predicted = torch.max(outputs, 1)\n    \n    # Plot\n    fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n    axes = axes.ravel()\n    \n    for i in range(num_images):\n        img = images[i].cpu().squeeze()\n        axes[i].imshow(img, cmap='gray')\n        axes[i].set_title(f'Pred: {predicted[i].item()} | True: {labels[i].item()}',\n                         color='green' if predicted[i] == labels[i] else 'red')\n        axes[i].axis('off')\n    \n    plt.tight_layout()\n    plt.savefig('predictions.png', dpi=150, bbox_inches='tight')\n    print('\\nPredictions saved to predictions.png')\n    plt.show()\n\n\ndef plot_training_history(train_losses, train_accs, test_losses, test_accs):\n    \"\"\"\n    Plot training history\n    \"\"\"\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n    \n    # Loss plot\n    ax1.plot(train_losses, label='Train Loss', color='#9D4EDD', linewidth=2)\n    ax1.plot(test_losses, label='Test Loss', color='#7209B7', linewidth=2)\n    ax1.set_xlabel('Epoch', fontsize=12)\n    ax1.set_ylabel('Loss', fontsize=12)\n    ax1.set_title('Training and Test Loss', fontsize=14, fontweight='bold')\n    ax1.legend()\n    ax1.grid(True, alpha=0.3)\n    \n    # Accuracy plot\n    ax2.plot(train_accs, label='Train Accuracy', color='#9D4EDD', linewidth=2)\n    ax2.plot(test_accs, label='Test Accuracy', color='#7209B7', linewidth=2)\n    ax2.set_xlabel('Epoch', fontsize=12)\n    ax2.set_ylabel('Accuracy (%)', fontsize=12)\n    ax2.set_title('Training and Test Accuracy', fontsize=14, fontweight='bold')\n    ax2.legend()\n    ax2.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.savefig('training_history.png', dpi=150, bbox_inches='tight')\n    print('Training history saved to training_history.png')\n    plt.show()\n\n\ndef main():\n    \"\"\"\n    Main training pipeline\n    \"\"\"\n    print('=' * 70)\n    print(' ' * 15 + 'MNIST CNN CLASSIFIER')\n    print(' ' * 10 + 'AI-DS-Masterpiece by Solvyr Eryx')\n    print('=' * 70)\n    \n    # Hyperparameters\n    batch_size = 64\n    learning_rate = 0.001\n    num_epochs = 10\n    \n    print(f'\\nHyperparameters:')\n    print(f'  Batch Size: {batch_size}')\n    print(f'  Learning Rate: {learning_rate}')\n    print(f'  Epochs: {num_epochs}')\n    print(f'  Device: {device}\\n')\n    \n    # Load data\n    print('Loading MNIST dataset...')\n    train_loader, test_loader = load_data(batch_size)\n    print(f'Training samples: {len(train_loader.dataset)}')\n    print(f'Test samples: {len(test_loader.dataset)}\\n')\n    \n    # Initialize model\n    model = EnhancedCNN().to(device)\n    print('Model Architecture:')\n    print(model)\n    print(f'\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\\n')\n    \n    # Loss and optimizer\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n    \n    # Training history\n    train_losses, train_accs = [], []\n    test_losses, test_accs = [], []\n    \n    # Training loop\n    print('Starting training...\\n')\n    start_time = time.time()\n    \n    for epoch in range(num_epochs):\n        print(f'\\nEpoch [{epoch+1}/{num_epochs}]')\n        print('-' * 70)\n        \n        # Train\n        train_loss, train_acc = train_model(model, train_loader, criterion, optimizer, device)\n        train_losses.append(train_loss)\n        train_accs.append(train_acc)\n        \n        # Evaluate\n        test_loss, test_acc = evaluate_model(model, test_loader, criterion, device)\n        test_losses.append(test_loss)\n        test_accs.append(test_acc)\n        \n        print(f'\\nTrain Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%')\n        print(f'Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.2f}%')\n    \n    training_time = time.time() - start_time\n    print(f'\\n{"=" * 70}')\n    print(f'Training completed in {training_time/60:.2f} minutes')\n    print(f'Final Test Accuracy: {test_accs[-1]:.2f}%')\n    print(f'{"=" * 70}\\n')\n    \n    # Save model\n    torch.save(model.state_dict(), 'mnist_cnn_model.pth')\n    print('Model saved to mnist_cnn_model.pth\\n')\n    \n    # Plot results\n    print('Generating visualizations...')\n    plot_training_history(train_losses, train_accs, test_losses, test_accs)\n    visualize_predictions(model, test_loader, device)\n    \n    print('\\n' + '=' * 70)\n    print(' ' * 20 + 'Training Complete!')\n    print('=' * 70)\n\n\nif __name__ == '__main__':\n    main()\n
